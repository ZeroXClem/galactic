{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from galactic import GalacticDataset\n",
    "\n",
    "# IMPORTANT! If you're using Galactic in a notebook, we use async coroutines to call APIs. You'll need these two lines or it will fail.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to demonstrate how to use the AI labeling and distillation features to clean and preprocess the OpenHermes instruction-tuning dataset. This is a great dataset for fine-tuning LLMs collected by [Teknium](https://github.com/teknium1)--but it's so big! We'll use Galactic's powerful tagging, labeling, and filtering features to reduce it down to a more manageable size, keeping only the topics we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = GalacticDataset.from_hugging_face(\n",
    "    \"teknium/openhermes\",\n",
    "    split=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['instruction', 'input', 'output', '__id'], 242831)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.column_names, len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do some basic stuff: We'll detect the language of each instruction, and then we'll filter out all the non-English instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Parameter 'function'=<function detect_language.<locals>.detect_ at 0x130536c10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e6768156be4847ae6028d83c23334f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/242831 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Detected language in field instruction, added language metadata to '__language'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c1b847c9594625a8cbce5440a3f3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/242831 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "242239"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.detect_language(\n",
    "    field=\"instruction\"\n",
    ").filter(\n",
    "    lambda x: x[\"__language\"] == \"en\"\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll count tokens of each instruction and output, and filter shorter outputs. We're interested in outputs where the model has to maintain fluency for a longer response, not just provide a short answer. Counting tokens with a tokenizer takes a while--if you don't provide a tokenizer, Galactic will just count bytes instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347874d32e6b48a4ae96519a8db541e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/242239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Counted tokens in fields: ['instruction', 'output'], added metadata to __token_count__\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe7848ce5c34bb8a07e7be658fa31ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/242239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "110355"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.count_tokens(\n",
    "    fields=[\"instruction\", \"output\"],\n",
    "    tokenizer=\"TaylorAI/Flash-Llama-13B\" # same as meta llama, but don't have to login to huggingface\n",
    ").filter(\n",
    "    lambda x: x[\"__token_count__instruction\"] > 10 and x[\"__token_count__output\"] > 225\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've significantly reduced the size of the dataset, we can start doing stuff that takes a little longer. To start out, we'll scan all the fields for PII and remove examples that contain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f50669abe943c8980c68733b53c5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Detected PII in fields: ['instruction', 'input', 'output']; added __pii__email, __pii__phone, __pii__credential, and __pii__any metadata.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01d67de6c714a19b38ebef0cf321b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/110355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "107639"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.detect_pii(\n",
    "    fields=[\"instruction\", \"input\", \"output\"],\n",
    ").filter(\n",
    "    lambda x: x[\"__pii__any\"] == False\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's think about the kinds of instructions we want our fine-tuning dataset to contain. OpenHermes includes instructions both for problem-solving, and also for more creative uses like writing and roleplay. Let's imagine we're only interested in problem-solving (math and programming). We'll use Galactic's AI classifier feature to automatically label topics with GPT-3.5-turbo on a fraction of the data. Then, we'll distill those labels to build a fast classifier that can cheaply and tractably classify the entire dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the api key and rate limits (check your account to see what your rate limits for GPT-3.5-turbo are)\n",
    "# Galactic will automatically use the 16k model for longer sequences, and the 4k model for shorter sequences.\n",
    "import os\n",
    "ds.set_openai_key(\n",
    "    os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "ds.set_rate_limits(\n",
    "    max_tokens_per_minute=350_000,\n",
    "    max_requests_per_minute=4_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6cb13185f94628b03aa73524df4ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Example prompt: Classify the provided text into one of the following classes:\n",
      "\n",
      "- programming: Request involves a programming task, including software, web development, data science, or machine learning.\n",
      "- math: Request involves a math task, including algebra, calculus, geometry, or statistics.\n",
      "- world_knowledge: Request involves a question about the world, including science, history, geography, or politics.\n",
      "- creative: Request involves a creative task, including writing, art, music, video, or storytelling.\n",
      "- roleplay: Request asks the model to play the role a character, historical person, or existing person.\n",
      "- other: Request does not fall into any of the above categories.\n",
      "\n",
      "---\n",
      "\n",
      "Text: Famous inventors and their inventions: Identify five well-known inventors and briefly describe one of their most significant inventions.\n",
      "\n",
      "---\n",
      "\n",
      "Class:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d560d920fba34e2fa78dcc090a520376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Parallel processing complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c4062fff0c48c5bb0573b76b111cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = {\n",
    "    \"programming\": \"Request involves a programming task, including software, web development, data science, or machine learning.\",\n",
    "    \"math\": \"Request involves a math task, including algebra, calculus, geometry, or statistics.\",\n",
    "    \"world_knowledge\": \"Request involves a question about the world, including science, history, geography, or politics.\",\n",
    "    \"creative\": \"Request involves a creative task, including writing, art, music, video, or storytelling.\",\n",
    "    \"roleplay\": \"Request asks the model to play the role a character, historical person, or existing person.\",\n",
    "    \"other\": \"Request does not fall into any of the above categories.\",\n",
    "}\n",
    "\n",
    "labeled_subset = ds.select(range(5000)).ai_classifier(\n",
    "    new_column=\"topic\",\n",
    "    field=\"instruction\",\n",
    "    classes=classes,\n",
    "    prompt=None, # use the default prompt, which just provides the class definitions and asks the model to classify the text\n",
    "    backend=\"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, all the outputs are valid classes. That's because Galactic uses a logit bias trick to force the API model to output a valid class. This means you're guaranteed to get a result for every example. It's good to put \"other\" as an option so the model has an escape hatch if none of the classes seem to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'math': 1751,\n",
       "         'programming': 1617,\n",
       "         'world_knowledge': 969,\n",
       "         'creative': 557,\n",
       "         'other': 98,\n",
       "         'roleplay': 8})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labeled_subset[\"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even labeling a few thousand examples with OpenAI took a really long time, and I have higher rate limits than most people. Luckily, Galactic is designed to help you label way more data than OpenAI can handle, by distilling labels into a fast classifier. One option is to train a linear model on top of embeddings, but the embeddings also take a while (especially if you're computing them locally), so that's only recommended if you want to embed everything anyway. A faster option is to train a FastText model. It only takes a few minutes to train, and can be used to classify any text even if we haven't computed embeddings yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   11 Best score:  0.893360 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  11063\n",
      "Number of labels: 6\n",
      "Progress: 100.0% words/sec/thread:  390860 lr:  0.000000 avg.loss:  0.499063 ETA:   0h 0m 0s\n",
      "Progress: 100.0% words/sec/thread: 1367544 lr:  0.000000 avg.loss:  0.085236 ETA:   0h 0m 0s\n",
      "INFO: Test set accuracy: 0.8726042841037204\n",
      "INFO: Test set accuracy per-class: {'__label__roleplay': nan, '__label__world_knowledge': 0.7351351351351352, '__label__other': 0.3333333333333333, '__label__creative': 0.8953488372093024, '__label__programming': 0.9347079037800687, '__label__math': 0.8996865203761756}\n"
     ]
    }
   ],
   "source": [
    "labeled_subset.train_fasttext_classifier(\n",
    "    model_name=\"topic_classifier\",\n",
    "    save_dir=\"../local/fasttext_models\",\n",
    "    input_field=\"instruction\",\n",
    "    label_field=\"topic\",\n",
    "    target_model_size=\"1M\", # constraining model size\n",
    "    training_duration=120 # 2 minutes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the test accuracy, it's not perfect, but for curating data, the main thing is to get rid of *most* of the stuff you don't want, and keep *most* of the stuff you do. This works pretty well! Now, we can use the model we just trained to predict topics for all 100k+ examples, in less time than it took for OpenAI to label 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6a551159ec4c6fbac983aed38455ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/107639 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Applied classifier to field instruction, added result to topic.\n"
     ]
    }
   ],
   "source": [
    "ds = ds.fasttext_classifier(\n",
    "    new_column=\"topic\",\n",
    "    model_path=\"../local/fasttext_models/topic_classifier.ftz\",\n",
    "    field=\"instruction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'math': 45343,\n",
       "         'world_knowledge': 32436,\n",
       "         'programming': 20033,\n",
       "         'creative': 9362,\n",
       "         'other': 460,\n",
       "         'roleplay': 5})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ds[\"topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8a464633864db49fc688ce90221726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/107639 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "65376"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.filter(\n",
    "    lambda x: x[\"topic\"] in [\"math\", \"programming\"]\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save(\"../local/hermes_problem_solving.jsonl\", overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
