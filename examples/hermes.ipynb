{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from galactic import GalacticDataset\n",
    "\n",
    "# IMPORTANT! If you're using Galactic in a notebook, we use async coroutines to call APIs. You'll need these two lines or it will fail.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to demonstrate how to use the AI labeling and distillation features to clean and preprocess the OpenHermes instruction-tuning dataset. This is a great dataset for fine-tuning LLMs collected by [Teknium](https://github.com/teknium1)--but it's so big! We'll use Galactic's powerful tagging, labeling, and filtering features to reduce it down to a more manageable size, keeping only the topics we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70eb16867994a15a0eccdb48e3bced9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/242831 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = GalacticDataset.from_hugging_face(\n",
    "    \"teknium/openhermes\",\n",
    "    split=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['input', 'instruction', 'output', '__id'], 242831)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.column_names, len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do some basic stuff: We'll detect the language of each instruction, and then we'll filter out all the non-English instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0557b24ff20e4ae2a5f181f2f084315d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/242239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Detected language in field instruction, added language metadata to '__language'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31a19f6ece4459ca6045c5d74da261e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/242239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "242239"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.detect_language(\n",
    "    field=\"instruction\"\n",
    ").filter(\n",
    "    lambda x: x[\"__language\"] == \"en\"\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll count tokens of each instruction and output, and filter shorter outputs. We're interested in outputs where the model has to maintain fluency for a longer response, not just provide a short answer. Counting tokens with a tokenizer takes a while--if you don't provide a tokenizer, Galactic will just count bytes instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361e56c57a054820b0d582db566f387a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/242239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Counted tokens in fields: ['instruction', 'output'], added metadata to __token_count__\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc34a7eb837447b6bb2470583913a82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/242239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "110355"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.count_tokens(\n",
    "    fields=[\"instruction\", \"output\"],\n",
    "    tokenizer=\"TaylorAI/Flash-Llama-13B\" # same as meta llama, but don't have to login to huggingface\n",
    ").filter(\n",
    "    lambda x: x[\"__token_count__instruction\"] > 10 and x[\"__token_count__output\"] > 225\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've significantly reduced the size of the dataset, we can start doing stuff that takes a little longer. To start out, we'll scan all the fields for PII and remove examples that contain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bb87f1f08a44c298d5abb165cda527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Detected PII in fields: ['instruction', 'input', 'output']; added __pii__email, __pii__phone, __pii__credential, and __pii__any metadata.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60264f25bfcd4d559154f01d92aafff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/110355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "107639"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.detect_pii(\n",
    "    fields=[\"instruction\", \"input\", \"output\"],\n",
    ").filter(\n",
    "    lambda x: x[\"__pii__any\"] == False\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's think about the kinds of instructions we want our fine-tuning dataset to contain. OpenHermes includes instructions both for problem-solving, and also for more creative uses like writing and roleplay. Let's imagine we're only interested in problem-solving (math and programming). We'll use Galactic's AI classifier feature to automatically label topics with GPT-3.5-turbo on a fraction of the data. Then, we'll distill those labels to build a fast classifier that can cheaply and tractably classify the entire dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the api key and rate limits (check your account to see what your rate limits for GPT-3.5-turbo are)\n",
    "# Galactic will automatically use the 16k model for longer sequences, and the 4k model for shorter sequences.\n",
    "ds.set_openai_key(\n",
    "    \"[...]\"\n",
    ")\n",
    "ds.set_rate_limits(\n",
    "    max_tokens_per_minute=350_000,\n",
    "    max_requests_per_minute=4_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7771004a0a469b934ffb8b4fef8b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Example prompt: Classify the provided text into one of the following classes:\n",
      "\n",
      "- programming: Request involves a programming task, including software, web development, data science, or machine learning.\n",
      "- math: Request involves a math task, including algebra, calculus, geometry, or statistics.\n",
      "- world_knowledge: Request involves a question about the world, including science, history, geography, or politics.\n",
      "- creative: Request involves a creative task, including writing, art, music, video, or storytelling.\n",
      "- roleplay: Request asks the model to play the role a character, historical person, or existing person.\n",
      "- other: Request does not fall into any of the above categories.\n",
      "\n",
      "---\n",
      "\n",
      "Text: Famous inventors and their inventions: Identify five well-known inventors and briefly describe one of their most significant inventions.\n",
      "\n",
      "---\n",
      "\n",
      "Class:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d0bbdf2ac1434b845c55623db076dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Parallel processing complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a110e06dedf4986ad77d96d5dabf691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = {\n",
    "    \"programming\": \"Request involves a programming task, including software, web development, data science, or machine learning.\",\n",
    "    \"math\": \"Request involves a math task, including algebra, calculus, geometry, or statistics.\",\n",
    "    \"world_knowledge\": \"Request involves a question about the world, including science, history, geography, or politics.\",\n",
    "    \"creative\": \"Request involves a creative task, including writing, art, music, video, or storytelling.\",\n",
    "    \"roleplay\": \"Request asks the model to play the role a character, historical person, or existing person.\",\n",
    "    \"other\": \"Request does not fall into any of the above categories.\",\n",
    "}\n",
    "\n",
    "labeled_subset = ds.select(range(5000)).ai_classifier(\n",
    "    \"topic\",\n",
    "    field=\"instruction\",\n",
    "    classes=classes,\n",
    "    prompt=None, # use the default prompt, which just provides the class definitions and asks the model to classify the text\n",
    "    backend=\"openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, all the outputs are valid classes. That's because Galactic uses a logit bias trick to force the API model to output a valid class. This means you're guaranteed to get a result for every example. It's good to put \"other\" as an option so the model has an escape hatch if none of the classes seem to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'math': 1742,\n",
       "         'programming': 1617,\n",
       "         'world_knowledge': 985,\n",
       "         'creative': 552,\n",
       "         'other': 96,\n",
       "         'roleplay': 8})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labeled_subset[\"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even labeling a few thousand examples with OpenAI took a really long time, and I have higher rate limits than most people. Luckily, Galactic is designed to help you label way more data than OpenAI can handle, by distilling labels into a fast classifier. One option is to train a linear model on top of embeddings, but the embeddings also take a while (especially if you're computing them locally), so that's only recommended if you want to embed everything anyway. A faster option is to train a FastText model. It only takes a few minutes to train, and can be used to classify any text even if we haven't computed embeddings yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   22 Best score:  0.897704 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  12097\n",
      "Number of labels: 6\n",
      "Progress: 100.0% words/sec/thread:   94656 lr:  0.000000 avg.loss:  0.661008 ETA:   0h 0m 0s\n",
      "Progress: 100.0% words/sec/thread:  281997 lr:  0.000000 avg.loss:  0.221580 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "labeled_subset.distill_fasttext(\n",
    "    model_name=\"topic_classifier\",\n",
    "    save_dir=\"../local/fasttext_models\",\n",
    "    input_field=\"instruction\",\n",
    "    label_field=\"topic\",\n",
    "    target_model_size=\"1M\", # constraining model size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the model we just trained to predict topics for all 100k+ examples, in less time than it took for OpenAI to label 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36562dff2214489b58851d3afd2b8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/107639 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Applied classifier to field instruction, added result to topic.\n",
      "INFO: Applied classifier to field instruction, added result to topic.\n"
     ]
    }
   ],
   "source": [
    "ds = ds.fasttext_classifier(\n",
    "    ds,\n",
    "    new_column=\"topic\",\n",
    "    model_path=\"../local/fasttext_models/topic_classifier.ftz\",\n",
    "    field=\"instruction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'math': 47069,\n",
       "         'world_knowledge': 32435,\n",
       "         'programming': 19150,\n",
       "         'creative': 8880,\n",
       "         'roleplay': 103,\n",
       "         'other': 2})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ds[\"topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291a7073ce6b4bee813c2c3879c121c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/66219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "66219"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.filter(\n",
    "    lambda x: x[\"topic\"] in [\"math\", \"programming\"]\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  input                                        instruction  \\\n",
       "0        A hotel chain wants to optimize its pricing st...   \n",
       "1        A zoo wants to expand its facilities by adding...   \n",
       "2        Implement a Scala program that reads a JSON fi...   \n",
       "3        A hospital wants to improve its emergency resp...   \n",
       "4        Write a Python script that connects to a Maria...   \n",
       "5        Solve the heat equation u_t = k * u_xx on the ...   \n",
       "6        Determine the critical points of the function ...   \n",
       "7        Write an OCaml function that finds the longest...   \n",
       "8        Create a Node.js Express application with two ...   \n",
       "9        Find the orthogonal projection of vector (1,2,...   \n",
       "\n",
       "                                              output  __id __language  \\\n",
       "0  To develop a dynamic pricing model for the hot...     5         en   \n",
       "1  Step 1: Calculate the total cost of expansion ...     9         en   \n",
       "2  To implement this program, you can use the fol...    15         en   \n",
       "3  Let x be the number of additional ambulances n...    16         en   \n",
       "4  ```python\\nimport argparse\\nimport mariadb\\nim...    21         en   \n",
       "5  To solve the heat equation with the given boun...    23         en   \n",
       "6  To find the critical points of the function g(...    25         en   \n",
       "7  let lcs_length s1 s2 =\\n  let m = String.lengt...    26         en   \n",
       "8  To create a Node.js Express application with t...    29         en   \n",
       "9  To find the orthogonal projection of a vector ...    30         en   \n",
       "\n",
       "   __token_count__instruction  __token_count__output  __pii__email  \\\n",
       "0                         121                    571         False   \n",
       "1                          86                    406         False   \n",
       "2                          56                    862         False   \n",
       "3                          94                    308         False   \n",
       "4                          59                    955         False   \n",
       "5                          56                    952         False   \n",
       "6                          45                    383         False   \n",
       "7                          18                    397         False   \n",
       "8                          57                    853         False   \n",
       "9                          27                    312         False   \n",
       "\n",
       "   __pii__phone  __pii__credential  __pii__any        topic  \n",
       "0         False              False       False  programming  \n",
       "1         False              False       False         math  \n",
       "2         False              False       False  programming  \n",
       "3         False              False       False         math  \n",
       "4         False              False       False  programming  \n",
       "5         False              False       False         math  \n",
       "6         False              False       False         math  \n",
       "7         False              False       False  programming  \n",
       "8         False              False       False  programming  \n",
       "9         False              False       False         math  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save(\"../local/hermes_problem_solving.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
